{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz5h-Uq_lcSF"
      },
      "source": [
        "# **Redes Neuronales**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkNBUrijleWn"
      },
      "source": [
        "**Equipo:**\n",
        "* Gabriel Frisancho\n",
        "* Jesús Niño\n",
        "* Adrian Sandoval\n",
        "* Diana Chavez"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLl5Hlnwd06w"
      },
      "source": [
        "## **Objetivo**\n",
        "\n",
        "Implementar manualmente el backpropagation en un Perceptrón Multicapa (MLP) para demostrar el Teorema de Aproximación Universal.  \n",
        "Se inicia con la clásica función XOR y se avanza hacia un problema real de regresión con el dataset **Airfoil Self‑Noise** de la NASA.  \n",
        "El reto pone a prueba la capacidad de los MLP para aproximar relaciones no lineales *sin ayuda de librerías de derivación* y los anima a experimentar con arquitecturas bajo un límite de 5 000 parámetros.\n",
        "\n",
        "## **Tareas**\n",
        "\n",
        "1. **Validación con XOR**  \n",
        "   - Construir y entrenar un MLP desde cero para reproducir la tabla de verdad XOR.  \n",
        "   - Evaluar el desempeño con Error Cuadrático Medio (MSE) y mostrar el límite de decisión.\n",
        "\n",
        "2. **Predicción de ruido aerodinámico (Airfoil)**  \n",
        "   - Preprocesar y dividir el dataset 70/15/15.  \n",
        "   - Diseñar la arquitectura, entrenar y optimizar el MLP para minimizar **RMSE**.  \n",
        "   - Mantener el total de parámetros **< 5 000**.  \n",
        "   - Incluir verificación de gradiente numérico en 10 muestras aleatorias.\n",
        "\n",
        "3. **Análisis y comparación**  \n",
        "   - Graficar las curvas de entrenamiento/validación.  \n",
        "   - Comparar contra una Regresión Lineal base.  \n",
        "   - Discutir evidencias que respalden el Teorema de Aproximación Universal.\n",
        "\n",
        "## **Entregables**\n",
        "\n",
        "1. **Canvas**  \n",
        "   - Notebook (.ipynb) con código, verificación de gradientes, curvas y análisis crítico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-j0V6nHAcBt"
      },
      "source": [
        "#### **Importar librerias y dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tgHyAx9Afol",
        "outputId": "8e65109b-453f-49a3-d11b-50d984c6aada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (1503, 5) (1503, 1)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import urllib.request, os, copy, math, random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat\"\n",
        "fname = \"airfoil_self_noise.dat\"\n",
        "if not os.path.exists(fname):\n",
        "    urllib.request.urlretrieve(url, fname)\n",
        "\n",
        "data = np.loadtxt(fname)\n",
        "X = data[:, :5]\n",
        "y = data[:, 5:]\n",
        "\n",
        "print(\"Dataset shape:\", X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F3UFPqAoxLj",
        "outputId": "653da643-6c71-453a-fbb0-cc60ac272ffb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Splits: (1052, 5) (225, 5) (226, 5)\n"
          ]
        }
      ],
      "source": [
        "# Train / val / test split\n",
        "rng = np.random.default_rng(42)\n",
        "idx = rng.permutation(len(X))\n",
        "n_train = int(0.7 * len(X))\n",
        "n_val = int(0.85 * len(X))\n",
        "train_idx, val_idx, test_idx = idx[:n_train], idx[n_train:n_val], idx[n_val:]\n",
        "\n",
        "X_train, y_train = X[train_idx], y[train_idx]\n",
        "X_val, y_val = X[val_idx], y[val_idx]\n",
        "X_test, y_test = X[test_idx], y[test_idx]\n",
        "\n",
        "# Normalize\n",
        "x_mean, x_std = X_train.mean(0, keepdims=True), X_train.std(0, keepdims=True)\n",
        "y_mean, y_std = y_train.mean(), y_train.std()\n",
        "\n",
        "def norm_x(x): return (x - x_mean) / x_std\n",
        "def norm_y(t): return (t - y_mean) / y_std\n",
        "def denorm_y(tn): return tn * y_std + y_mean\n",
        "\n",
        "X_train, X_val, X_test = map(norm_x, (X_train, X_val, X_test))\n",
        "y_train_n, y_val_n, y_test_n = map(norm_y, (y_train, y_val, y_test))\n",
        "print(\"Splits:\", X_train.shape, X_val.shape, X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lx7EzhXuDdjL"
      },
      "source": [
        "## **1. Capas bases**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2DW0jvgBo7Hb"
      },
      "outputs": [],
      "source": [
        "# CODE HERE for backward pass\n",
        "\n",
        "# f0, f1, f2\n",
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        self.W = np.random.randn(in_dim, out_dim) * np.sqrt(2.0 / in_dim)\n",
        "        self.b = np.zeros((1, out_dim))\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return x @ self.W + self.b\n",
        "    def backward(self, d_out):\n",
        "        self.grad_W = self.x.T @ d_out\n",
        "        self.grad_b = np.sum(d_out, axis=0, keepdims=True)\n",
        "        return d_out @ self.W.T\n",
        "\n",
        "# h0, h1, h2, ....\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        self.mask = x > 0\n",
        "        return x * self.mask\n",
        "    def backward(self, d_out):\n",
        "        return d_out * self.mask\n",
        "\n",
        "# error\n",
        "class MSELoss:\n",
        "    def forward(self, pred, target):\n",
        "        self.diff = pred - target\n",
        "        return np.mean(self.diff ** 2)\n",
        "    def backward(self):\n",
        "        n = np.prod(self.diff.shape)\n",
        "        return 2 * self.diff / n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diK4zkTAo_FL"
      },
      "source": [
        "# **2. Red MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B3jP9c5No-gr"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, dims):\n",
        "        self.layers = []\n",
        "        for i in range(len(dims)-2):\n",
        "            self.layers.append(Linear(dims[i], dims[i+1])) #fi\n",
        "            self.layers.append(ReLU()) # hi\n",
        "        self.layers.append(Linear(dims[-2], dims[-1]))\n",
        "\n",
        "    # CODE HERE (forward, backward pass)\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer.forward(x)\n",
        "        return x\n",
        "    \n",
        "    def backward(self, d_out):\n",
        "        for layer in reversed(self.layers):\n",
        "            d_out = layer.backward(d_out)\n",
        "        return d_out\n",
        "\n",
        "    @property\n",
        "    def n_params(self):\n",
        "        total = 0\n",
        "        for l in self.layers:\n",
        "            if isinstance(l, Linear):\n",
        "                total += l.W.size + l.b.size\n",
        "        return total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdSEu6pkpGxM"
      },
      "source": [
        "# **3. Gradient check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uj6CcXTLpGMz"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rel error 2.2716180264578438e-11\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "np.True_"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def grad_check(model, x, y, eps=1e-5, tol=1e-4):\n",
        "    loss_fn = MSELoss()\n",
        "    pred = model.forward(x)\n",
        "    loss = loss_fn.forward(pred, y)\n",
        "    grad = loss_fn.backward()\n",
        "    model.backward(grad)\n",
        "    lin = next(l for l in model.layers if isinstance(l, Linear))\n",
        "    i, j = np.random.randint(lin.W.shape[0]), np.random.randint(lin.W.shape[1])\n",
        "    orig = lin.W[i, j]\n",
        "    lin.W[i, j] = orig + eps\n",
        "    plus = loss_fn.forward(model.forward(x), y)\n",
        "    lin.W[i, j] = orig - eps\n",
        "    minus = loss_fn.forward(model.forward(x), y)\n",
        "    lin.W[i, j] = orig\n",
        "    num_grad = (plus - minus) / (2*eps)\n",
        "    ana_grad = lin.grad_W[i, j]\n",
        "    rel_err = abs(num_grad - ana_grad) / max(1e-8, abs(num_grad)+abs(ana_grad))\n",
        "    print('rel error', rel_err)\n",
        "    return rel_err < tol\n",
        "\n",
        "tmp = MLP([5,8,4,1])\n",
        "grad_check(tmp, X_train[:10], y_train_n[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXboyuDRpObW"
      },
      "source": [
        "# **4. Entrenamiento**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "L6LJWNOxpP2m"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parámetros totales: 3009\n",
            "Epoch 001 | Train Loss: 0.919476 | Val Loss: 0.696282\n",
            "Epoch 010 | Train Loss: 0.425537 | Val Loss: 0.355793\n",
            "Epoch 020 | Train Loss: 0.350280 | Val Loss: 0.310517\n",
            "Epoch 030 | Train Loss: 0.305890 | Val Loss: 0.289012\n",
            "Epoch 040 | Train Loss: 0.274456 | Val Loss: 0.259839\n",
            "Epoch 050 | Train Loss: 0.259083 | Val Loss: 0.257866\n",
            "Epoch 060 | Train Loss: 0.234992 | Val Loss: 0.236586\n",
            "Epoch 070 | Train Loss: 0.221771 | Val Loss: 0.238983\n",
            "Epoch 080 | Train Loss: 0.212193 | Val Loss: 0.243501\n",
            "Epoch 090 | Train Loss: 0.206163 | Val Loss: 0.220227\n",
            "Epoch 100 | Train Loss: 0.189904 | Val Loss: 0.212033\n",
            "Epoch 110 | Train Loss: 0.186057 | Val Loss: 0.202340\n",
            "Epoch 120 | Train Loss: 0.174883 | Val Loss: 0.206856\n",
            "Epoch 130 | Train Loss: 0.168389 | Val Loss: 0.195278\n",
            "Epoch 140 | Train Loss: 0.165867 | Val Loss: 0.193958\n",
            "Epoch 150 | Train Loss: 0.152081 | Val Loss: 0.195408\n",
            "Epoch 160 | Train Loss: 0.147669 | Val Loss: 0.199055\n",
            "Epoch 170 | Train Loss: 0.143966 | Val Loss: 0.180652\n",
            "Epoch 180 | Train Loss: 0.136562 | Val Loss: 0.183680\n",
            "Epoch 190 | Train Loss: 0.133404 | Val Loss: 0.174794\n",
            "Epoch 200 | Train Loss: 0.127553 | Val Loss: 0.167033\n",
            "Epoch 210 | Train Loss: 0.133738 | Val Loss: 0.174998\n",
            "Epoch 220 | Train Loss: 0.122537 | Val Loss: 0.189373\n",
            "Epoch 230 | Train Loss: 0.117386 | Val Loss: 0.171657\n",
            "Epoch 240 | Train Loss: 0.125097 | Val Loss: 0.373023\n",
            "Epoch 250 | Train Loss: 0.109632 | Val Loss: 0.160685\n",
            "Epoch 260 | Train Loss: 0.107569 | Val Loss: 0.179956\n",
            "Epoch 270 | Train Loss: 0.112318 | Val Loss: 0.162160\n",
            "Epoch 280 | Train Loss: 0.105698 | Val Loss: 0.161952\n",
            "Epoch 290 | Train Loss: 0.102432 | Val Loss: 0.168357\n",
            "Epoch 300 | Train Loss: 0.104793 | Val Loss: 0.162024\n",
            "Epoch 310 | Train Loss: 0.093956 | Val Loss: 0.157461\n",
            "Epoch 320 | Train Loss: 0.099235 | Val Loss: 0.180386\n",
            "Epoch 330 | Train Loss: 0.100648 | Val Loss: 0.157693\n",
            "Epoch 340 | Train Loss: 0.094378 | Val Loss: 0.170606\n",
            "Epoch 350 | Train Loss: 0.101096 | Val Loss: 0.147276\n",
            "Epoch 360 | Train Loss: 0.097632 | Val Loss: 0.156809\n",
            "Epoch 370 | Train Loss: 0.089782 | Val Loss: 0.137496\n",
            "Epoch 380 | Train Loss: 0.093682 | Val Loss: 0.173982\n",
            "Epoch 390 | Train Loss: 0.091142 | Val Loss: 0.142378\n",
            "Epoch 400 | Train Loss: 0.095160 | Val Loss: 0.147383\n",
            "Epoch 410 | Train Loss: 0.080554 | Val Loss: 0.157271\n",
            "Epoch 420 | Train Loss: 0.084708 | Val Loss: 0.136422\n",
            "Epoch 430 | Train Loss: 0.080942 | Val Loss: 0.158018\n",
            "Epoch 440 | Train Loss: 0.089789 | Val Loss: 0.132664\n",
            "Epoch 450 | Train Loss: 0.075663 | Val Loss: 0.153097\n",
            "Epoch 460 | Train Loss: 0.075469 | Val Loss: 0.126294\n",
            "Epoch 470 | Train Loss: 0.084234 | Val Loss: 0.128948\n",
            "Epoch 480 | Train Loss: 0.072042 | Val Loss: 0.124068\n",
            "Epoch 490 | Train Loss: 0.070802 | Val Loss: 0.122391\n",
            "Epoch 500 | Train Loss: 0.073533 | Val Loss: 0.154385\n"
          ]
        }
      ],
      "source": [
        "dims = [5, 64, 32, 16, 1]\n",
        "lr = 0.01\n",
        "epochs = 500\n",
        "batch = 64\n",
        "model = MLP(dims)\n",
        "print('Parámetros totales:', model.n_params)\n",
        "loss_fn = MSELoss()\n",
        "train_hist, val_hist = [], []\n",
        "\n",
        "for ep in range(1, epochs + 1):\n",
        "    perm = np.random.permutation(len(X_train))\n",
        "    X_train_shuff = X_train[perm]\n",
        "    y_train_shuff = y_train_n[perm]\n",
        "\n",
        "    losses = []\n",
        "    for i in range(0, len(X_train), batch):\n",
        "        xb = X_train_shuff[i:i+batch]\n",
        "        yb = y_train_shuff[i:i+batch]\n",
        "\n",
        "        pred = model.forward(xb)\n",
        "        loss = loss_fn.forward(pred, yb)\n",
        "        losses.append(loss)\n",
        "\n",
        "        d_out = loss_fn.backward()\n",
        "        model.backward(d_out)\n",
        "\n",
        "        for layer in model.layers:\n",
        "            if isinstance(layer, Linear):\n",
        "                layer.W -= lr * layer.grad_W\n",
        "                layer.b -= lr * layer.grad_b\n",
        "\n",
        "    train_hist.append(np.mean(losses))\n",
        "\n",
        "    pred_val = model.forward(X_val)\n",
        "    val_loss = loss_fn.forward(pred_val, y_val_n)\n",
        "    val_hist.append(val_loss)\n",
        "\n",
        "    if ep % 10 == 0 or ep == 1:\n",
        "        print(f\"Epoch {ep:03d} | Train Loss: {train_hist[-1]:.6f} | Val Loss: {val_hist[-1]:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NseP4IxNp3bN"
      },
      "source": [
        "# **5. XOR Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5v0DoRpwp_mK"
      },
      "outputs": [],
      "source": [
        "# CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vQIhlY_paWH"
      },
      "source": [
        "# **6. Evaluación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsSIAlJYpcLR"
      },
      "outputs": [],
      "source": [
        "te_pred = model.forward(X_test)\n",
        "test_rmse = np.sqrt(np.mean((denorm_y(te_pred)-y_test)**2))\n",
        "print('Test RMSE:', test_rmse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxsSLMwSpdW-"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(train_hist))*10+1, train_hist, label='Train')\n",
        "plt.plot(np.arange(len(val_hist))*10+1, val_hist, label='Val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.title('Curva de aprendizaje')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ut4qk9hpft2"
      },
      "source": [
        "# **7. Análisis crítico**\n",
        "- Discute influencia de arquitectura, overfitting, Universal Approximation, etc."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

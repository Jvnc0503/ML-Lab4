{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Redes Neuronales**"
      ],
      "metadata": {
        "id": "dz5h-Uq_lcSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo:**\n",
        "* Integrante 1 (XX%)\n",
        "* Integrante 2 (XX%)\n",
        "* Integrante 3 (XX%)\n",
        "* Integrante 4 (XX%)"
      ],
      "metadata": {
        "id": "RkNBUrijleWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Objetivo**\n",
        "\n",
        "Implementar manualmente el backpropagation en un Perceptrón Multicapa (MLP) para demostrar el Teorema de Aproximación Universal.  \n",
        "Se inicia con la clásica función XOR y se avanza hacia un problema real de regresión con el dataset **Airfoil Self‑Noise** de la NASA.  \n",
        "El reto pone a prueba la capacidad de los MLP para aproximar relaciones no lineales *sin ayuda de librerías de derivación* y los anima a experimentar con arquitecturas bajo un límite de 5 000 parámetros.\n",
        "\n",
        "## **Tareas**\n",
        "\n",
        "1. **Validación con XOR**  \n",
        "   - Construir y entrenar un MLP desde cero para reproducir la tabla de verdad XOR.  \n",
        "   - Evaluar el desempeño con Error Cuadrático Medio (MSE) y mostrar el límite de decisión.\n",
        "\n",
        "2. **Predicción de ruido aerodinámico (Airfoil)**  \n",
        "   - Preprocesar y dividir el dataset 70/15/15.  \n",
        "   - Diseñar la arquitectura, entrenar y optimizar el MLP para minimizar **RMSE**.  \n",
        "   - Mantener el total de parámetros **< 5 000**.  \n",
        "   - Incluir verificación de gradiente numérico en 10 muestras aleatorias.\n",
        "\n",
        "3. **Análisis y comparación**  \n",
        "   - Graficar las curvas de entrenamiento/validación.  \n",
        "   - Comparar contra una Regresión Lineal base.  \n",
        "   - Discutir evidencias que respalden el Teorema de Aproximación Universal.\n",
        "\n",
        "## **Entregables**\n",
        "\n",
        "1. **Canvas**  \n",
        "   - Notebook (.ipynb) con código, verificación de gradientes, curvas y análisis crítico."
      ],
      "metadata": {
        "id": "hLl5Hlnwd06w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Importar librerias y dataset**"
      ],
      "metadata": {
        "id": "0-j0V6nHAcBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import urllib.request, os, copy, math, random\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat\"\n",
        "fname = \"airfoil_self_noise.dat\"\n",
        "if not os.path.exists(fname):\n",
        "    urllib.request.urlretrieve(url, fname)\n",
        "\n",
        "data = np.loadtxt(fname)\n",
        "X = data[:, :5]\n",
        "y = data[:, 5:]\n",
        "\n",
        "print(\"Dataset shape:\", X.shape, y.shape)"
      ],
      "metadata": {
        "id": "3tgHyAx9Afol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e65109b-453f-49a3-d11b-50d984c6aada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset shape: (1503, 5) (1503, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train / val / test split\n",
        "rng = np.random.default_rng(42)\n",
        "idx = rng.permutation(len(X))\n",
        "n_train = int(0.7 * len(X))\n",
        "n_val = int(0.85 * len(X))\n",
        "train_idx, val_idx, test_idx = idx[:n_train], idx[n_train:n_val], idx[n_val:]\n",
        "\n",
        "X_train, y_train = X[train_idx], y[train_idx]\n",
        "X_val, y_val = X[val_idx], y[val_idx]\n",
        "X_test, y_test = X[test_idx], y[test_idx]\n",
        "\n",
        "# Normalize\n",
        "x_mean, x_std = X_train.mean(0, keepdims=True), X_train.std(0, keepdims=True)\n",
        "y_mean, y_std = y_train.mean(), y_train.std()\n",
        "\n",
        "def norm_x(x): return (x - x_mean) / x_std\n",
        "def norm_y(t): return (t - y_mean) / y_std\n",
        "def denorm_y(tn): return tn * y_std + y_mean\n",
        "\n",
        "X_train, X_val, X_test = map(norm_x, (X_train, X_val, X_test))\n",
        "y_train_n, y_val_n, y_test_n = map(norm_y, (y_train, y_val, y_test))\n",
        "print(\"Splits:\", X_train.shape, X_val.shape, X_test.shape)"
      ],
      "metadata": {
        "id": "_F3UFPqAoxLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "653da643-6c71-453a-fbb0-cc60ac272ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splits: (1052, 5) (225, 5) (226, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Capas bases**"
      ],
      "metadata": {
        "id": "lx7EzhXuDdjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE HERE for backward pass\n",
        "\n",
        "# f0, f1, f2\n",
        "class Linear:\n",
        "    def __init__(self, in_dim, out_dim):\n",
        "        self.W = np.random.randn(in_dim, out_dim) * np.sqrt(2.0 / in_dim)\n",
        "        self.b = np.zeros((1, out_dim))\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        return x @ self.W + self.b\n",
        "\n",
        "# h0, h1, h2, ....\n",
        "class ReLU:\n",
        "    def forward(self, x):\n",
        "        self.mask = x > 0\n",
        "        return x * self.mask\n",
        "\n",
        "# error\n",
        "class MSELoss:\n",
        "    def forward(self, pred, target):\n",
        "        self.diff = pred - target\n",
        "        return np.mean(self.diff ** 2)"
      ],
      "metadata": {
        "id": "2DW0jvgBo7Hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Red MLP**"
      ],
      "metadata": {
        "id": "diK4zkTAo_FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP:\n",
        "    def __init__(self, dims):\n",
        "        self.layers = []\n",
        "        for i in range(len(dims)-2):\n",
        "            self.layers.append(Linear(dims[i], dims[i+1])) #fi\n",
        "            self.layers.append(ReLU()) # hi\n",
        "        self.layers.append(Linear(dims[-2], dims[-1]))\n",
        "\n",
        "    # CODE HERE (forward, backward pass)\n",
        "\n",
        "    @property\n",
        "    def n_params(self):\n",
        "        total = 0\n",
        "        for l in self.layers:\n",
        "            if isinstance(l, Linear):\n",
        "                total += l.W.size + l.b.size\n",
        "        return total"
      ],
      "metadata": {
        "id": "B3jP9c5No-gr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Gradient check**"
      ],
      "metadata": {
        "id": "BdSEu6pkpGxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_check(model, x, y, eps=1e-5, tol=1e-4):\n",
        "    loss_fn = MSELoss()\n",
        "    pred = model.forward(x)\n",
        "    loss = loss_fn.forward(pred, y)\n",
        "    grad = loss_fn.backward()\n",
        "    model.backward(grad)\n",
        "    lin = next(l for l in model.layers if isinstance(l, Linear))\n",
        "    i, j = np.random.randint(lin.W.shape[0]), np.random.randint(lin.W.shape[1])\n",
        "    orig = lin.W[i, j]\n",
        "    lin.W[i, j] = orig + eps\n",
        "    plus = loss_fn.forward(model.forward(x), y)\n",
        "    lin.W[i, j] = orig - eps\n",
        "    minus = loss_fn.forward(model.forward(x), y)\n",
        "    lin.W[i, j] = orig\n",
        "    num_grad = (plus - minus) / (2*eps)\n",
        "    ana_grad = lin.grad_W[i, j]\n",
        "    rel_err = abs(num_grad - ana_grad) / max(1e-8, abs(num_grad)+abs(ana_grad))\n",
        "    print('rel error', rel_err)\n",
        "    return rel_err < tol\n",
        "\n",
        "tmp = MLP([5,8,4,1])\n",
        "grad_check(tmp, X_train[:10], y_train_n[:10])"
      ],
      "metadata": {
        "id": "uj6CcXTLpGMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Entrenamiento**"
      ],
      "metadata": {
        "id": "rXboyuDRpObW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dims = [5, 64, 32, 16, 1]\n",
        "lr = 0.01\n",
        "epochs = 500\n",
        "batch = 64\n",
        "model = MLP(dims)\n",
        "print('Parámetros totales:', model.n_params)\n",
        "loss_fn = MSELoss()\n",
        "train_hist, val_hist = [], []\n",
        "\n",
        "for ep in range(1, epochs+1):\n",
        "  # CODE HERE\n",
        "  pass"
      ],
      "metadata": {
        "id": "L6LJWNOxpP2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. XOR Validation**"
      ],
      "metadata": {
        "id": "NseP4IxNp3bN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CODE HERE"
      ],
      "metadata": {
        "id": "5v0DoRpwp_mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Evaluación**"
      ],
      "metadata": {
        "id": "9vQIhlY_paWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "te_pred = model.forward(X_test)\n",
        "test_rmse = np.sqrt(np.mean((denorm_y(te_pred)-y_test)**2))\n",
        "print('Test RMSE:', test_rmse)"
      ],
      "metadata": {
        "id": "SsSIAlJYpcLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(np.arange(len(train_hist))*10+1, train_hist, label='Train')\n",
        "plt.plot(np.arange(len(val_hist))*10+1, val_hist, label='Val')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('RMSE')\n",
        "plt.legend()\n",
        "plt.title('Curva de aprendizaje')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BxsSLMwSpdW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Análisis crítico**\n",
        "- Discute influencia de arquitectura, overfitting, Universal Approximation, etc."
      ],
      "metadata": {
        "id": "4Ut4qk9hpft2"
      }
    }
  ]
}